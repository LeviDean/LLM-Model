{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"http_proxy\"] = \"http://127.0.0.1:8889\"\n",
    "os.environ[\"https_proxy\"] = \"http://127.0.0.1:8889\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # 1 Import\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, TrainingArguments, Trainer, DefaultDataCollator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 Load dataset\n",
    "# datasets = load_dataset('cmrc2018', cache_dir=\"./datasets\")\n",
    "# datasets.save_to_disk(\"./datasets/local/cmrc2018\")\n",
    "datasets = load_from_disk(\"./datasets/local/cmrc2018\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 data preprocessing\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"hfl/chinese-macbert-base\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dataset = datasets[\"train\"].select(range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_samples = tokenizer(text=sample_dataset[\"question\"],\n",
    "                              text_pair=sample_dataset[\"context\"],\n",
    "                              max_length=384,\n",
    "                              return_overflowing_tokens=True,\n",
    "                              stride=128,\n",
    "                              return_offsets_mapping=True,\n",
    "                              truncation=\"only_second\",\n",
    "                              padding=\"max_length\",)\n",
    "\n",
    "tokenized_samples.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_samples.overflow_to_sample_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenized_samples.overflow_to_sample_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sen in tokenizer.batch_decode(tokenized_samples[\"input_ids\"][:4]):\n",
    "    print(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_mapping = tokenized_samples.pop(\"overflow_to_sample_mapping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, _ in enumerate(sample_mapping):\n",
    "    answer = sample_dataset[\"answers\"][sample_mapping[idx]]\n",
    "    start_char = answer[\"answer_start\"][0]\n",
    "    end_char = start_char + len(answer[\"text\"][0])\n",
    "    # locate the token position of answer\n",
    "    \n",
    "    context_start = tokenized_samples.sequence_ids(idx).index(1)\n",
    "    context_end = tokenized_samples.sequence_ids(idx).index(None, context_start) - 1\n",
    "    \n",
    "    offset = tokenized_samples[\"offset_mapping\"][idx]\n",
    "    \n",
    "    # if the answer is in the context\n",
    "    if offset[context_end][1] < start_char or offset[context_start][0] > end_char:\n",
    "        start_token_pos = 0\n",
    "        end_token_pos = 0\n",
    "    else:\n",
    "        token_id = context_start\n",
    "        while token_id <= context_end and offset[token_id][0] < start_char:\n",
    "            token_id += 1\n",
    "        start_token_pos = token_id \n",
    "            \n",
    "        token_id = context_end\n",
    "        while token_id >= start_token_pos and offset[token_id][1] > end_char:\n",
    "            token_id -= 1\n",
    "        end_token_pos = token_id\n",
    "    \n",
    "    print(answer, start_char, end_char, context_start, context_end, start_token_pos, end_token_pos) \n",
    "    print(\"decode:\", tokenizer.decode(tokenized_samples[\"input_ids\"][idx][start_token_pos:end_token_pos+1]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_func(examples):\n",
    "    tokenized_examples = tokenizer(text=examples[\"question\"],\n",
    "                              text_pair=examples[\"context\"],\n",
    "                              max_length=384,\n",
    "                              return_offsets_mapping=True,\n",
    "                              return_overflowing_tokens=True,\n",
    "                              stride=128,\n",
    "                              truncation=\"only_second\",\n",
    "                              padding=\"max_length\",)\n",
    "    \n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    \n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    \n",
    "    example_ids = []\n",
    "    \n",
    "    for idx, _ in enumerate(sample_mapping):\n",
    "        answer = examples[\"answers\"][sample_mapping[idx]]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = start_char + len(answer[\"text\"][0])\n",
    "        # locate the token position of answer\n",
    "        context_start = tokenized_examples.sequence_ids(idx).index(1)\n",
    "        context_end = tokenized_examples.sequence_ids(idx).index(None, context_start) - 1\n",
    "        \n",
    "        offset = tokenized_examples[\"offset_mapping\"][idx]\n",
    "        \n",
    "        # if the answer is in the context\n",
    "        if offset[context_end][1] < start_char or offset[context_start][0] > end_char:\n",
    "            start_token_pos = 0\n",
    "            end_token_pos = 0\n",
    "        else:\n",
    "            token_id = context_start\n",
    "            while token_id <= context_end and offset[token_id][0] < start_char:\n",
    "                token_id += 1\n",
    "            start_token_pos = token_id \n",
    "                \n",
    "            token_id = context_end\n",
    "            while token_id >= context_end and offset[token_id][1] > end_char:\n",
    "                token_id -= 1\n",
    "            end_token_pos = token_id\n",
    "            \n",
    "        start_positions.append(start_token_pos)\n",
    "        end_positions.append(end_token_pos)\n",
    "        example_ids.append(examples[\"id\"][sample_mapping[idx]])\n",
    "        tokenized_examples[\"offset_mapping\"][idx] = [\n",
    "            (o if tokenized_examples.sequence_ids(idx)[k] == 1 else None)\n",
    "            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][idx])\n",
    "        ]\n",
    "    \n",
    "    tokenized_examples[\"start_positions\"] = start_positions\n",
    "    tokenized_examples[\"end_positions\"] = end_positions\n",
    "    tokenized_examples[\"example_ids\"] = example_ids\n",
    "    \n",
    "    return tokenized_examples\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = datasets.map(process_func, batched=True, remove_columns=datasets[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets[\"train\"][\"offset_mapping\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get model output\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "def get_result(start_logits, end_logits, examples, features):\n",
    "    predictions = {}\n",
    "    references = {}\n",
    "    \n",
    "    # example id to features\n",
    "    example_to_features = collections.defaultdict(list)\n",
    "    for idx, example_id in enumerate(features[\"example_ids\"]):\n",
    "        example_to_features[example_id].append(idx)\n",
    "        \n",
    "    # best answer candidate\n",
    "    n_best = 2\n",
    "    # max answer length\n",
    "    max_answer_length = 30\n",
    "    \n",
    "    for example in examples:\n",
    "        example_id = example[\"id\"]\n",
    "        context = example[\"context\"]\n",
    "        answers =[]\n",
    "        for feature_idx in example_to_features[example_id]:\n",
    "            start_logit = start_logits[feature_idx]\n",
    "            end_logit = end_logits[feature_idx]\n",
    "            offset = features[feature_idx][\"offset_mapping\"]\n",
    "            start_indexes = np.argsort(start_logit)[::-1][:n_best].tolist()\n",
    "            end_indexes = np.argsort(end_logit)[::-1][:n_best].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    if offset[start_index] is None or offset[end_index] is None:\n",
    "                        continue\n",
    "                    if end_index < start_index:\n",
    "                        continue\n",
    "                    if end_index - start_index + 1 > max_answer_length:\n",
    "                        continue\n",
    "                    answers.append({\n",
    "                        \"text\":context[offset[start_index][0]:offset[end_index][1]],\n",
    "                        \"score\": start_logit[start_index] + end_logit[end_index],\n",
    "                    })\n",
    "        if len(answers) > 0:\n",
    "            best_answer = max(answers, key=lambda x: x[\"score\"])\n",
    "            predictions[example_id] = best_answer[\"text\"]\n",
    "        else:\n",
    "            predictions[example_id] = \"\"\n",
    "            \n",
    "        references[example_id] = example[\"answers\"][\"text\"]\n",
    "        \n",
    "    return predictions, references\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 Evaluation function\n",
    "from cmrc_eval import evaluate_cmrc\n",
    "\n",
    "def metric(pred):\n",
    "    start_logits, end_logits = pred[0]\n",
    "    if start_logits.shape[0] == len(tokenized_datasets[\"validation\"]):\n",
    "        p, r = get_result(start_logits=start_logits,\n",
    "                          end_logits=end_logits,\n",
    "                          examples=datasets[\"validation\"],\n",
    "                          features=tokenized_datasets[\"validation\"])\n",
    "    else:\n",
    "        p, r = get_result(start_logits=start_logits,\n",
    "                          end_logits=end_logits,\n",
    "                          examples=datasets[\"test\"],\n",
    "                          features=tokenized_datasets[\"test\"])\n",
    "    print('123')\n",
    "    return evaluate_cmrc(p, r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 Create Model\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"hfl/chinese-macbert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6 TrainArgs\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./models/models_for_qa\",\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=64,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_strategy=\"epoch\",\n",
    "    # load_best_model_at_end=True,\n",
    "    logging_steps=50,\n",
    "    logging_strategy=\"steps\",\n",
    "    num_train_epochs=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7 Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    data_collator=DefaultDataCollator(),\n",
    "    compute_metrics=metric,\n",
    ")\n",
    "\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8 Train\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8 pipeline\n",
    "\n",
    "from transformers import pipeline\n",
    "pipe = pipeline(\"question-answering\", model=model, tokenizer=tokenizer, device=0)\n",
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
